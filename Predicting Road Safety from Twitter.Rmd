
---
title: "Predicting Road Safety from Twitter"
author: "Winston Saunders"
date: "Dec 2014"
output: html_document
---

#####Analysis of winter road safety probabilities for Santiam Pass, Oregon_

###Summary

The [Oregon Dept of Transportation](https://tripcheck.com/Pages/Twitter.asp) regularly publishes, as a public service, live road reports via Twitter. The record created by these tweets is a great public service since drivers can get real time information about road conditions.  

However, this data can also be repurposed to recontruct events on specific sections of road over a longer baseline and to correlate them with road conditions, etc. for statistical analysis. That is the purpose of this analysis.

This analysis can benefit drivers. The "average" citizen has today few ways to learn or benefit from historical data about road safety. However, as better navigation tools and even self-driving cars become available, analysis of historical data may not only improve travel times, but might also be used to improve travel safety through the choice of driving routes, driving time, or avoiding certain road conditions.

This article seeks to understand whether twitter data can be used for this purpose and to develop a predictive machine learning algorithm.

Analysis focuses on a specific location, US Highway 20 at Santiam Pass, a 4800 foot (1450 meters) mountain pass in the Cascade Range (milepost 79) of Oregon. As the main route from the Central Oregon city of Bend to the Willamette Valley cities of Portland, Eugene, and Salem, Highway20 has high traffic year round and is the site of [frequent](http://www.nuggetnews.com/archives/960717/front1.shtml) accidents. 

This analysis use historical data from Twitter to understand the correlation of accidents on this specific section of road to reported snowy road conditions. 

###Getting the Tweets

The twitter feed for this analysis is downloaded by a separate R program (Since I could not get RStudio to allow me to enter the required security features during runtime execution). The data are stored as a .csv of Tweets locally. 


```{r "get_tweets", echo=FALSE, warning=FALSE}

library(bitops)
library(RCurl)

##This program assumes you have already run the TwitterReader.R to download a feed timeline. 
##It reads the data in teh form of a .csv 

## Get the relevnat tweet data

        if (getwd()=="/Users/winstonsaunders/Documents") {setwd("TripcheckR")}
        if (getwd()!="/Users/winstonsaunders/Documents/TripcheckR") {setwd("/Users/winstonsaunders/Documents/TripcheckR")}
        file_name <- "TripCheckUS20B_Sep15"

        hwy_df<-read.csv(paste0(file_name, ".csv"))

        #print(hwy_df$date[1:5])
```

##Data Cleaning

```{r echo=FALSE}


        ##keep only columns of interest
        hwy_df<-hwy_df[,c("created", "text")]
        ##display sample of raw data
        #head(hwy_df)
        
        hwy_df$created<-as.character(hwy_df$created)
        hwy_df$text<-as.character(hwy_df$text)

        ##make the date column a date
        hwy_df$date<-as.Date(hwy_df$created)
        ## create pacific time column
        hwy_df$PDT<-as.POSIXct(hwy_df$created, tz="UTC")
        ## convert to Pacific time
        attributes(hwy_df$PDT)$tzone<-"US/Pacific"
        
        ##make an "hour" column (for time of day)
        library(lubridate)
        t.lub <- ymd_hms(hwy_df$PDT)
        hwy_df$hour <- hour(t.lub) + minute(t.lub)/60
        
        ## make a seconds column
        hwy_df$chronsecs<-as.numeric(as.POSIXlt(hwy_df$PDT))

        
        ## create "dayperiod"
        hwy_df$dayperiod<-NA
        hwy_df$dayperiod[hwy_df$hour>=3 & hwy_df$hour<9]<-"morning"
        hwy_df$dayperiod[hwy_df$hour>=9 & hwy_df$hour<15]<-"midday"
        hwy_df$dayperiod[hwy_df$hour>=15 & hwy_df$hour<21]<-"evening"
        hwy_df$dayperiod[hwy_df$hour>=19]<-"night"
        hwy_df$dayperiod[hwy_df$hour<3]<-"night"
        
        ## make a factor
        hwy_df$dayperiod<-as.factor(hwy_df$dayperiod)
        #head(hwy_df)
        
        

        ### capture start and edn times of period 
        tEnd <- as.Date(hwy_df$created)[length(hwy_df$created)]
        tStart <- as.Date(hwy_df$created[1])
        

```

The data analyzed cover the dates from `r tEnd` to `r tStart`. There are `r dim(hwy_df)[1]` tweets during this period. 

Data are cleaned by searching the text for the strings "crash" and "snow" and then filtered for location "Santiam Pass Summit". A few instances of cleaned and engineered date are shown above.


## Data Analysis

```{r "clean", echo=FALSE, warning=FALSE, message=FALSE}


        trip_incident_filter <- function(data_df, incident = "crash"){
        ##FUNCTION filtering text of tweets for incidents 
            ##accepts data frame requiring:
            ## text field to search
            
            ## returns:
            ## fitered data with added column called hourdelta (in hours)


            ##filter incidencts
            hwy_inc <-data_df[grep(tolower(incident), tolower(data_df$text)),]

            return(hwy_inc)
            }
            
        inc_dedupe <- function(hwy_inc){
            ## reduce duplicates
            ## add column hourdelta to df
            ## uses chronsecs
                ## reverse oder
                hwy_inc <- hwy_inc[rev(rownames(hwy_inc)),]
    
                ## compute time difference between entries
                hwy_inc$hourdelta<-c((3*3600+1),diff(hwy_inc$chronsecs, lag=1))/3600
        
        
                
                hwy_inc<-hwy_inc[hwy_inc$hourdelta>3, ]
                
                hwy_inc <- hwy_inc[rev(rownames(hwy_inc)),]
                
                
                return(hwy_inc)
                }       
        

        ##Filter
        
        
        hwy_crash<-trip_incident_filter(hwy_df, "crash")
        hwy_snow<-trip_incident_filter(hwy_df, "snow")
        
        hwy_crash_santiam<-trip_incident_filter(hwy_crash, "santiam")
        
        hwy_crash_santiam<-inc_dedupe(hwy_crash_santiam)
        
        
        library(ggplot2)
        
        p <- ggplot(hwy_crash_santiam, aes(x = hour)) + geom_histogram(fill="salmon", color="darkred", binwidth=1)
        p <- p + theme_bw()
        p <- p + ggtitle("Accidents on Santiam")
        print(p)
        
        p <- ggplot(hwy_crash, aes(x = hour)) + geom_histogram(fill="blue", color="darkblue", binwidth=0.5)
        p <- p + theme_bw()
        p <- p + ggtitle("All Hwy20 Crashes")
        print(p)
        
    
        ##Numbers of accidents adn snow for later use

        nAcc <- nrow(hwy_crash)
        nSnow <- nrow(hwy_snow)
        
        #nAcc_santiam <- nrow(hwy_crash_santiam)
        #nSnow_santiam <- nrow(hwy_snow_santiam)

   

```

###Data analysis and reduction

To analyze the data create a dataframe of dates from `r tStart` to `r tEnd` and then add two binary columns, one for whether a crash occured that day and whether snow was reported on the road. A few rows of the dataframe are shown.


```{r, echo=FALSE}

    snow_days<-hwy_snow$date
        
    hwy_crash_santiam$snow_day <- (hwy_crash_santiam$date %in% snow_days)
    hwy_crash_santiam$weekday <- weekdays(as.Date(hwy_crash_santiam$date, "1970-01-01"))


```

```{r, echo=FALSE}
    
    
    ##CREATE TREE ANALYSIS DATA FRAME  
    
    ## create a list of all dates
    tStart <- hwy_df$date[1]
    tEnd <- hwy_df$date[nrow(hwy_df)]

        ##Start with dates
        Tvect<-tStart:tEnd
        date<-as.Date(Tvect, "1970-01-01")
    
    Analysis_df<-as.data.frame(date)
    
    #head(Analysis_df)
    
    Analysis_df$crash <- (Analysis_df$date %in% hwy_crash_santiam$date)
    
    #Analysis_df$crash[Analysis_df$crash==TRUE]<-"crash"
    #Analysis_df$crash[ Analysis_df$crash==FALSE]<-"safe"
    #Analysis_df$crash<-as.character(Analysis_df$crash)
    #Analysis_df$crash<-as.factor(Analysis_df$crash)
    
    
    Analysis_df$snow <- (Analysis_df$date %in% hwy_snow$date)
    
    Analysis_df$snow[Analysis_df$snow==TRUE]<-"snow"
    Analysis_df$snow [Analysis_df$snow ==FALSE]<-"clear"
    Analysis_df$snow <-as.character(Analysis_df$snow )
    Analysis_df$snow <- as.factor(Analysis_df$snow )
    
    
    Analysis_df$weekday <- weekdays(as.Date(Analysis_df$date, "1970-01-01"))
    
    ##Holidays
        library(timeDate)
        holidays_list = holidayNYSE(2005:2015)
        holidays_df<-as.Date(holidays_list)
    Analysis_df$holiday <-  (Analysis_df$date %in% holidays_df)
    
    
    
    ## assign label
    #Analysis_df$holiday[Analysis_df$holiday == TRUE] <- "holiday"
    #Analysis_df$holiday[Analysis_df$holiday == FALSE] <- "nonholiday"
    #Analysis_df$holiday<-as.factor(Analysis_df$holiday)
    
    print(head(Analysis_df, 50))
    
    ## Add weekend
    Analysis_df$weekend <- Analysis_df$weekday == "Sunday" | Analysis_df$weekday == "Saturday"

    
    ## Long Weekend
    Analysis_df$longweekend<-rep(FALSE, nrow(Analysis_df))
    
    for (i in 3:(length(Analysis_df$date-3))) {
        ## Mondays
        if (Analysis_df[i, "weekday"] == "Monday" & Analysis_df[i,"holiday"] == TRUE){
            Analysis_df[i, "longweekend"] <- TRUE
            Analysis_df[i+1, "longweekend"] <- TRUE
            Analysis_df[i+2, "longweekend"] <- TRUE
            
        }
        
        ## Fridays
        if (Analysis_df[i, "weekday"] == "Friday" & Analysis_df[i, "holiday"] == TRUE){
            Analysis_df[i, "longweekend"] <- TRUE
            Analysis_df[i-1, "longweekend"] <- TRUE
            Analysis_df[i-2, "longweekend"] <- TRUE
            
        }
        
        ## Thursdays
        if (Analysis_df[i, "weekday"] == "Thursday" & Analysis_df[i, "holiday"] == TRUE){
            Analysis_df[i, "longweekend"] <- TRUE
            Analysis_df[i-1, "longweekend"] <- TRUE
            Analysis_df[i-2, "longweekend"] <- TRUE
            Analysis_df[i-2, "longweekend"] <- TRUE
            
        }
        
    }
     
    Analysis_df$weekday<-as.factor(Analysis_df$weekday)
    
    
```






```{r, echo=FALSE, fig.height=5, fig.width=9} 
        
        ##Fit the data
    
         ## get rid of holiday classifier
        Analysis_df_t<- subset(Analysis_df, select = -holiday )

        library(rpart)
        library(rpart.plot)
        treefit<-rpart(crash~snow+weekday+longweekend+holiday, Analysis_df, method="class", control=rpart.control(minsplit=5, cp=0, xval=5))
        

        prp(treefit, uniform=TRUE, main="Classification Tree for Crashes",type=0, extra=2, faclen=0)
        #text(treefit, use.n=TRUE, all=TRUE, cex=.7)
        
        treefit
```


<style>

table { 
    display: table;
    border-collapse: collapse;
    border-spacing: 10px;
    border-color: gray;
    background-color: #a1b2c3;
    align: center;
    font: 12px arial, sans-serif;
}
th, td {
    border: 2px solid black;
    padding: 5px;
    text-align: center;
}
</style>


```{r, echo=FALSE, results='asis'}
#         library(caret)
#         library(xtable)
#         
#         c_matrix<-confusionMatrix(Analysis_df$crash, Analysis_df$snow)
# 
#         c_matrix_class_df<-as.data.frame(c_matrix$byClass)
#         colnames(c_matrix_class_df)<-c("Value")
#     
#         print(xtable(c_matrix_class_df), type='html')
#         
#         c_matrix_class_df<-as.data.frame(c_matrix$table)
#     
#     
#         print(xtable(c_matrix_class_df), type='html')
# 
# 
#         print(c_matrix)

```
        
```{r}
  
darkviolet_x<-	#552683
darkyellow_x<-	#E7A922
white_x <-	#FFFFFF
gray_x<-	#A9A8A7
darkyellow_y<-	#CA8B01 
            
            
            
            
            
           
            
        
```